{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. CONFIG 用yaml\n",
    "# BUILD DATALOADER\n",
    "# 2. 生成xyz_27\n",
    "# 3. (注释版) 生成所有需要放入diffusion的东西"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from for_test import process_target, construct_contig, get_idx0_hotspots, get_init_xyz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PDB = '1a2y.pdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parse input pdb ###\n",
    "target_feats = process_target(INPUT_PDB, parse_hetatom=True, center=False)\n",
    "# dict_keys(['xyz_27', 'mask_27', 'seq', 'pdb_idx', 'xyz_het', 'info_het'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "352"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_feats['seq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contigmap:\n",
    "#   contigs: null\n",
    "#   inpaint_seq: null\n",
    "#   inpaint_str: null\n",
    "#   provide_seq: null\n",
    "#   length: null\n",
    "\n",
    "CONTIG_CONF = {\n",
    "    'contigs': ['352-352'],\n",
    "    'inpaint_seq': None,\n",
    "    'inpaint_str': None,\n",
    "    'provide_seq': None,\n",
    "    'length': None\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate specific contig ###\n",
    "\n",
    "# Generate a specific contig from the range of possibilities specified at input\n",
    "\n",
    "\n",
    "contig_map = construct_contig(target_feats, CONTIG_CONF)\n",
    "mappings = contig_map.get_mappings()\n",
    "mask_seq = torch.from_numpy(contig_map.inpaint_seq)[None,:]\n",
    "mask_str = torch.from_numpy(contig_map.inpaint_str)[None,:]\n",
    "binderlen =  len(contig_map.inpaint) # binderlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPI_CONF = {\n",
    "    'hotspot_res': None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get Hotspots ###\n",
    "\n",
    "hotspot_0idx = get_idx0_hotspots(mappings, PPI_CONF, binderlen) # None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialize other attributes ###\n",
    "\n",
    "# BASE VAR\n",
    "xyz_27 = target_feats['xyz_27']\n",
    "mask_27 = target_feats['mask_27']\n",
    "seq_orig = target_feats['seq'] # 352\n",
    "L_mapped = len(contig_map.ref)\n",
    "contig_map = contig_map\n",
    "\n",
    "# DIFFUSION VAR 4\n",
    "diffusion_mask = mask_str #[[False]]\n",
    "chain_idx = ['A' if i < binderlen else 'B' for i in range(L_mapped)] # ['A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate initial coordinates ###\n",
    "# Fully diffusing from points initialised at the origin\n",
    "# adjust size of input xt according to residue map\n",
    "\n",
    "xyz_mapped = torch.full((1,1,L_mapped,27,3), np.nan) \n",
    "xyz_mapped[:, :, contig_map.hal_idx0, ...] = xyz_27[contig_map.ref_idx0,...] \n",
    "xyz_motif_prealign = xyz_mapped.clone() \n",
    "motif_prealign_com = xyz_motif_prealign[0,0,:,1].mean(dim=0)\n",
    "motif_com = xyz_27[contig_map.ref_idx0,1].mean(dim=0)\n",
    "\n",
    "# DIFFUSION VAR 1\n",
    "xyz_mapped = get_init_xyz(xyz_mapped).squeeze() # torch.Size([352, 27, 3])\n",
    "\n",
    "# adjust the size of the input atom map\n",
    "atom_mask_mapped = torch.full((L_mapped, 27), False)\n",
    "\n",
    "# DIFFUSION VAR 3\n",
    "atom_mask_mapped[contig_map.hal_idx0] = mask_27[contig_map.ref_idx0] # torch.Size([352, 27])\n",
    "\n",
    "# Diffuse the contig-mapped coordinates \n",
    "t_step_input = 50 # int(diffuser_conf.T)\n",
    "\n",
    "# DIFFUSION VAR 5\n",
    "t_list = np.arange(1, t_step_input+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate initial sequence ###\n",
    "\n",
    "seq_t = torch.full((1,L_mapped), 21).squeeze() # 21 is the mask token\n",
    "seq_t[contig_map.hal_idx0] = seq_orig[contig_map.ref_idx0]\n",
    "\n",
    "# # Unmask sequence if desired\n",
    "# if self._conf.contigmap.provide_seq is not None:\n",
    "#     seq_t[self.mask_seq.squeeze()] = seq_orig[self.mask_seq.squeeze()] \n",
    "\n",
    "seq_t[~mask_seq.squeeze()] = 21\n",
    "\n",
    "# DIFFUSION VAR 2\n",
    "seq_t    = torch.nn.functional.one_hot(seq_t, num_classes=22).float() # [L,22]\n",
    "seq_orig = torch.nn.functional.one_hot(seq_orig, num_classes=22).float() # [L,22]\n",
    "\n",
    "# fa_stack, xyz_true = self.diffuser.diffuse_pose(\n",
    "#     xyz_mapped,\n",
    "#     torch.clone(seq_t),\n",
    "#     atom_mask_mapped.squeeze(),\n",
    "#     diffusion_mask=self.diffusion_mask.squeeze(),\n",
    "#     t_list=t_list)\n",
    "# xT = fa_stack[-1].squeeze()[:,:14,:]\n",
    "# xt = torch.clone(xT)\n",
    "\n",
    "# self.denoiser = self.construct_denoiser(len(self.contig_map.ref), visible=self.mask_seq.squeeze())\n",
    "\n",
    "#         return xt, seq_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #####################################\n",
    "# ### Initialise Potentials Manager ###\n",
    "# #####################################\n",
    "\n",
    "# self.potential_manager = PotentialManager(self.potential_conf,\n",
    "#                                             self.ppi_conf,\n",
    "#                                             self.diffuser_conf,\n",
    "#                                             self.inf_conf,\n",
    "#                                             self.hotspot_0idx,\n",
    "#                                             self.binderlen)\n",
    "\n",
    "\n",
    "    \n",
    "# ####################################\n",
    "# ### Generate initial coordinates ###\n",
    "# ####################################\n",
    "\n",
    "# if self.diffuser_conf.partial_T: # None\n",
    "#     assert xyz_27.shape[0] == L_mapped, f\"there must be a coordinate in the input PDB for \\\n",
    "#             each residue implied by the contig string for partial diffusion.  length of \\\n",
    "#             input PDB != length of contig string: {xyz_27.shape[0]} != {L_mapped}\"\n",
    "#     assert contig_map.hal_idx0 == contig_map.ref_idx0, f'for partial diffusion there can \\\n",
    "#             be no offset between the index of a residue in the input and the index of the \\\n",
    "#             residue in the output, {contig_map.hal_idx0} != {contig_map.ref_idx0}'\n",
    "#     # Partially diffusing from a known structure\n",
    "#     xyz_mapped=xyz_27\n",
    "#     atom_mask_mapped = mask_27\n",
    "# else:\n",
    "#     # Fully diffusing from points initialised at the origin\n",
    "#     # adjust size of input xt according to residue map\n",
    "#     xyz_mapped = torch.full((1,1,L_mapped,27,3), np.nan)\n",
    "#     xyz_mapped[:, :, contig_map.hal_idx0, ...] = xyz_27[contig_map.ref_idx0,...]\n",
    "#     xyz_motif_prealign = xyz_mapped.clone()\n",
    "#     motif_prealign_com = xyz_motif_prealign[0,0,:,1].mean(dim=0)\n",
    "#     self.motif_com = xyz_27[contig_map.ref_idx0,1].mean(dim=0)\n",
    "#     xyz_mapped = get_init_xyz(xyz_mapped).squeeze()\n",
    "#     # adjust the size of the input atom map\n",
    "#     atom_mask_mapped = torch.full((L_mapped, 27), False)\n",
    "#     atom_mask_mapped[contig_map.hal_idx0] = mask_27[contig_map.ref_idx0]\n",
    "\n",
    "    # # Diffuse the contig-mapped coordinates \n",
    "    # if self.diffuser_conf.partial_T:\n",
    "    #     assert self.diffuser_conf.partial_T <= self.diffuser_conf.T, \"Partial_T must be less than T\"\n",
    "    #     self.t_step_input = int(self.diffuser_conf.partial_T)\n",
    "    # else:\n",
    "    #     self.t_step_input = int(self.diffuser_conf.T)\n",
    "    # t_list = np.arange(1, self.t_step_input+1)\n",
    "\n",
    "    # #################################\n",
    "    # ### Generate initial sequence ###\n",
    "    # #################################\n",
    "\n",
    "    # seq_t = torch.full((1,L_mapped), 21).squeeze() # 21 is the mask token\n",
    "    # seq_t[contig_map.hal_idx0] = seq_orig[contig_map.ref_idx0]\n",
    "    \n",
    "    # # Unmask sequence if desired\n",
    "    # if self._conf.contigmap.provide_seq is not None:\n",
    "    #     seq_t[self.mask_seq.squeeze()] = seq_orig[self.mask_seq.squeeze()] \n",
    "\n",
    "    # seq_t[~self.mask_seq.squeeze()] = 21\n",
    "    # seq_t    = torch.nn.functional.one_hot(seq_t, num_classes=22).float() # [L,22]\n",
    "    # seq_orig = torch.nn.functional.one_hot(seq_orig, num_classes=22).float() # [L,22]\n",
    "\n",
    "    # fa_stack, xyz_true = self.diffuser.diffuse_pose(\n",
    "    #     xyz_mapped,\n",
    "    #     torch.clone(seq_t),\n",
    "    #     atom_mask_mapped.squeeze(),\n",
    "    #     diffusion_mask=self.diffusion_mask.squeeze(),\n",
    "    #     t_list=t_list)\n",
    "    # xT = fa_stack[-1].squeeze()[:,:14,:]\n",
    "    # xt = torch.clone(xT)\n",
    "\n",
    "    # self.denoiser = self.construct_denoiser(len(self.contig_map.ref), visible=self.mask_seq.squeeze())\n",
    "\n",
    "    # ######################\n",
    "    # ### Apply Symmetry ###\n",
    "    # ######################\n",
    "\n",
    "    # if self.symmetry is not None:\n",
    "    #     xt, seq_t = self.symmetry.apply_symmetry(xt, seq_t)\n",
    "    # self._log.info(f'Sequence init: {seq2chars(torch.argmax(seq_t, dim=-1))}')\n",
    "    \n",
    "    # self.msa_prev = None\n",
    "    # self.pair_prev = None\n",
    "    # self.state_prev = None\n",
    "\n",
    "# #########################################\n",
    "# ### Parse ligand for ligand potential ###\n",
    "# #########################################\n",
    "\n",
    "# if self.potential_conf.guiding_potentials is not None:\n",
    "#     if any(list(filter(lambda x: \"substrate_contacts\" in x, self.potential_conf.guiding_potentials))):\n",
    "#         assert len(self.target_feats['xyz_het']) > 0, \"If you're using the Substrate Contact potential, \\\n",
    "#                 you need to make sure there's a ligand in the input_pdb file!\"\n",
    "#         het_names = np.array([i['name'].strip() for i in self.target_feats['info_het']])\n",
    "#         xyz_het = self.target_feats['xyz_het'][het_names == self._conf.potentials.substrate]\n",
    "#         xyz_het = torch.from_numpy(xyz_het)\n",
    "#         assert xyz_het.shape[0] > 0, f'expected >0 heteroatoms from ligand with name {self._conf.potentials.substrate}'\n",
    "#         xyz_motif_prealign = xyz_motif_prealign[0,0][self.diffusion_mask.squeeze()]\n",
    "#         motif_prealign_com = xyz_motif_prealign[:,1].mean(dim=0)\n",
    "#         xyz_het_com = xyz_het.mean(dim=0)\n",
    "#         for pot in self.potential_manager.potentials_to_apply:\n",
    "#             pot.motif_substrate_atoms = xyz_het\n",
    "#             pot.diffusion_mask = self.diffusion_mask.squeeze()\n",
    "#             pot.xyz_motif = xyz_motif_prealign\n",
    "#             pot.diffuser = self.diffuser"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
